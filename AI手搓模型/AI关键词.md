AI学习领域的知识体系庞杂，**不存在官方统一的200个关键词**，但结合基础理论、核心技术、工具框架、应用场景等维度，可整理出200个高频核心术语，覆盖从入门到进阶的全链路知识。以下按模块分类并逐一解说：

# 一、AI基础概念类（15个）
| 关键词 | 解说 |
|--------|------|
| 人工智能（AI） | 旨在让机器模拟人类智能的技术科学，涵盖感知、推理、学习、决策等能力，是整个领域的统称 |
| 机器学习（ML） | AI的核心分支，让机器通过数据自动学习规律、优化模型，无需人工编写固定规则 |
| 深度学习（DL） | 机器学习的子领域，基于多层神经网络（深度神经网络）处理复杂数据（如图像、文本），是大模型的技术基础 |
| 强化学习（RL） | 机器学习分支，智能体通过与环境交互“试错”，以奖励/惩罚机制优化决策策略 |
| 自然语言处理（NLP） | 研究机器理解和生成人类语言的技术，涵盖文本分类、翻译、对话等场景 |
| 计算机视觉（CV） | 让机器“看懂”图像/视频的技术，包括目标检测、图像分割、人脸识别等任务 |
| 语音识别（ASR） | 将人类语音信号转化为文字的技术，是智能音箱、实时字幕的核心支撑 |
| 语音合成（TTS） | 把文字转化为自然语音的技术，常见于导航、有声读物等场景 |
| 多模态AI | 融合文本、图像、语音、视频等多种数据类型的AI技术，可实现跨模态理解与生成 |
| 智能体（Agent） | 能自主感知环境、做出决策并执行动作的AI实体，可独立完成复杂任务（如AutoGPT） |
| 知识图谱 | 以图结构存储实体和关系的知识库（如“张三-朋友-李四”），用于知识推理和问答 |
| 数据挖掘 | 从海量数据中提取隐藏规律和价值的技术，常与机器学习结合使用 |
| 模式识别 | 让机器识别数据中特定模式的技术，是CV、NLP、ASR的底层逻辑之一 |
| 专家系统 | 早期AI技术，基于专家规则库实现特定领域决策（如医疗诊断），灵活性低于机器学习模型 |
| 神经符号AI | 融合神经网络（感知能力）和符号逻辑（推理能力）的技术，兼顾数据驱动与逻辑解释性 |

# 二、机器学习基础理论类（20个）
| 关键词 | 解说 |
|--------|------|
| 监督学习 | 训练数据含“输入+标签”，模型学习输入到标签的映射关系（如分类、回归），是最常用的学习范式 |
| 无监督学习 | 训练数据无标签，模型自主挖掘数据的内在结构（如聚类、降维），用于探索性数据分析 |
| 半监督学习 | 结合少量标注数据和大量未标注数据训练模型，平衡标注成本与模型性能 |
| 自监督学习 | 无监督学习的进阶范式，模型从数据本身生成“伪标签”（如图像补全、文本掩码）完成自我训练 |
| 泛化能力 | 模型在**未见过的新数据**上的预测能力，泛化能力强代表模型鲁棒性好，不过拟合 |
| 过拟合 | 模型过度拟合训练数据的噪声，导致在新数据上表现极差，可通过正则化、数据增强缓解 |
| 欠拟合 | 模型复杂度不足，无法捕捉数据的核心规律，导致训练集和测试集表现都差，可通过增加模型层数、特征维度解决 |
| 特征 | 数据中可用于模型学习的关键属性（如图像的边缘、文本的词频），特征质量决定模型上限 |
| 特征工程 | 对原始数据进行筛选、转换、组合以生成优质特征的过程，传统机器学习中至关重要 |
| 标签 | 监督学习中数据的“标准答案”（如分类任务的类别、回归任务的数值） |
| 样本 | 训练/测试模型的单条数据（如图像分类中一张带标签的图片） |
| 数据集 | 样本的集合，通常分为训练集（模型学习）、验证集（调参）、测试集（评估性能） |
| 分布 | 数据的统计规律（如正态分布、均匀分布），若训练集与测试集分布差异大，模型泛化能力会下降 |
| 假设空间 | 模型所有可能的预测函数的集合，模型训练就是从假设空间中筛选最优函数 |
| 归纳偏好 | 模型在多个等效假设中选择的“偏好”（如决策树优先选信息增益大的特征），保证模型输出唯一性 |
| 奥卡姆剃刀 | 机器学习的核心原则：在效果相近的模型中，优先选择结构更简单的模型，降低过拟合风险 |
| 独立同分布（i.i.d.） | 假设训练集和测试集的样本相互独立且服从同一分布，是传统机器学习模型的核心前提 |
| 因果推断 | 研究变量间因果关系的技术，区别于“相关性”，可用于模型的可解释性和决策优化 |
| 迁移学习 | 将预训练模型在A任务学到的知识迁移到B任务，降低B任务的标注成本和训练难度 |
| 增量学习 | 模型在新数据上持续学习，同时保留旧知识的能力，解决“灾难性遗忘”问题 |

# 三、经典机器学习算法类（25个）
| 关键词 | 解说 |
|--------|------|
| 线性回归 | 基础回归算法，假设输入与输出呈线性关系，通过拟合直线预测连续数值（如房价预测） |
| 逻辑回归 | 用于二分类任务的线性模型，通过Sigmoid函数将输出映射到0-1区间，代表分类概率 |
| 决策树 | 基于树形结构做决策的模型，每个节点对应一个特征判断，叶子节点为预测结果，解释性强 |
| 随机森林 | 集成多个决策树的模型，通过“随机抽样特征+样本”降低单棵树的过拟合，提升稳定性 |
| 梯度提升树（GBDT） | 集成学习算法，按顺序训练多棵决策树，每棵树修正前一棵树的误差，代表模型有XGBoost、LightGBM |
| XGBoost | 优化版GBDT，引入正则化和并行训练，是工业界经典的结构化数据建模工具 |
| LightGBM | 轻量级梯度提升树，采用直方图算法和单边采样，训练速度快、内存占用低 |
| 支持向量机（SVM） | 分类/回归算法，通过寻找“最大间隔超平面”划分样本，高维数据下表现优异，适合小数据集 |
| K近邻算法（KNN） | 惰性学习算法，无训练过程，预测时找最近的K个样本的多数类别/均值作为结果，简单但计算成本高 |
| K均值聚类（K-Means） | 经典无监督聚类算法，将数据划分为K个簇，使簇内样本相似度高、簇间相似度低 |
| 层次聚类 | 无监督聚类算法，通过“自底向上聚合”或“自顶向下拆分”构建聚类树，无需预设簇数量 |
| DBSCAN | 密度聚类算法，可识别任意形状的簇，还能标记噪声点，适合异常检测 |
| 主成分分析（PCA） | 无监督降维算法，通过线性变换将高维数据映射到低维，保留数据核心方差 |
| 线性判别分析（LDA） | 监督式降维算法，降维时兼顾类内紧凑性和类间分离性，更适合分类任务 |
| 朴素贝叶斯 | 基于贝叶斯定理的分类算法，假设特征间相互独立，适合文本分类（如垃圾邮件识别） |
| 隐马尔可夫模型（HMM） | 序列建模算法，用于处理时序数据（如语音识别、词性标注），假设当前状态仅依赖前一状态 |
| 条件随机场（CRF） | 序列标注算法，可利用全局特征优化标注结果，常用于命名实体识别、句法分析 |
| 关联规则挖掘 | 无监督算法，挖掘数据中“同时出现”的关联关系（如“买尿布的人常买啤酒”），代表算法Apriori |
| Apriori算法 | 经典关联规则算法，通过逐层迭代筛选频繁项集，实现关联规则挖掘 |
| 协同过滤 | 推荐算法核心，基于用户/物品的相似性做推荐（如“和你相似的用户也喜欢XX”） |
| 岭回归 | 带L2正则化的线性回归，通过惩罚权重的平方和缓解过拟合 |
| Lasso回归 | 带L1正则化的线性回归，可将不重要特征的权重置零，实现特征选择 |
| 弹性网络（ElasticNet） | 融合L1和L2正则化的线性回归，兼顾特征选择和模型稳定性 |
| 感知机 | 最简单的二分类神经网络，仅含一层神经元，是深度学习的雏形 |
| 贝叶斯网络 | 基于概率图的建模工具，用有向图表示变量间的概率依赖关系，用于不确定性推理 |

# 四、深度学习核心技术类（30个）
| 关键词 | 解说 |
|--------|------|
| 神经网络 | 模仿人脑神经元结构的计算模型，由输入层、隐藏层、输出层组成，是深度学习的基础 |
| 深度神经网络（DNN） | 包含多层隐藏层的神经网络，可学习数据的复杂非线性特征 |
| 卷积神经网络（CNN） | 专为处理网格数据（图像/视频）设计的网络，通过卷积层提取局部特征，具备平移不变性 |
| 循环神经网络（RNN） | 处理时序数据的网络，通过“记忆单元”保留历史信息，但存在梯度消失/爆炸问题 |
| 长短期记忆网络（LSTM） | 改进版RNN，通过门控单元（输入门、遗忘门、输出门）解决长序列依赖问题，适合文本、语音时序建模 |
| 门控循环单元（GRU） | 简化版LSTM，合并遗忘门和输入门，参数更少、训练更快，性能接近LSTM |
| 注意力机制 | 让模型“聚焦”数据中关键信息的技术（如翻译时关注对应单词），是Transformer的核心 |
| Transformer | 2017年提出的网络架构，基于自注意力机制和编码器-解码器结构，彻底革新NLP，是大模型的基础 |
| 自注意力机制 | 注意力机制的变体，同一序列内的元素相互计算注意力权重（如文本中不同单词的关联） |
| 多头注意力 | 并行计算多个自注意力头，让模型同时关注不同维度的特征，提升表达能力 |
| 编码器-解码器 | 经典序列建模结构，编码器处理输入序列，解码器生成输出序列（如机器翻译） |
| 残差连接（Residual Connection） | 深度学习网络的优化技巧，将浅层特征直接传递到深层，缓解梯度消失，支持构建超深网络 |
| 批量归一化（BN） | 对每层输入做标准化处理，加速模型收敛，提升稳定性，降低过拟合风险 |
| 激活函数 | 为神经网络引入非线性的函数，让模型可学习复杂映射，常见的有ReLU、Sigmoid、Tanh、Softmax |
| ReLU | 最常用的激活函数，计算简单（x>0时输出x，否则输出0），有效缓解梯度消失 |
| Softmax | 多分类任务输出层激活函数，将输出转化为0-1的概率分布，总和为1 |
| 池化层 | CNN中的下采样层，通过最大池化/平均池化降低特征维度，保留关键信息 |
| 全连接层（FC） | 神经网络中神经元两两连接的层，用于整合特征并输出最终结果 |
| 嵌入层（Embedding Layer） | 将离散符号（如单词）转化为低维稠密向量的层，是NLP模型的输入层核心 |
| 卷积层 | CNN的核心层，通过卷积核滑动提取图像的局部特征（如边缘、纹理） |
| 反卷积层 | 也称转置卷积，用于将低维特征映射回高维，常用于图像分割、生成式模型 |
| 生成对抗网络（GAN） | 生成式模型，由生成器和判别器博弈训练，可生成逼真数据（如图像、文本） |
| 变分自编码器（VAE） | 基于概率的生成式模型，通过编码器生成潜在分布，解码器从分布中采样生成数据 |
| 注意力图 | 可视化注意力机制的结果，展示模型关注数据的哪些区域（如图像的目标、文本的关键词） |
| 位置编码 | Transformer中为序列添加位置信息的模块（因自注意力无顺序感知），让模型识别序列顺序 |
| 归一化 | 数据预处理或网络层优化手段，除BN外还有层归一化（LN）、实例归一化（IN）等 |
| 层归一化（LN） | 对单个样本的特征维度做归一化，适合NLP的序列数据，稳定性优于BN |
| 梯度下降 | 模型优化的核心算法，通过沿梯度反方向更新参数，最小化损失函数 |
| 反向传播 | 计算损失函数对网络参数梯度的算法，是梯度下降的实现基础，支持参数迭代更新 |
| 损失函数 | 衡量模型预测值与真实值差异的函数，是模型优化的目标，常见的有MSE、CrossEntropy |

# 五、大模型专项术语类（30个）
| 关键词 | 解说 |
|--------|------|
| 大语言模型（LLM） | 基于Transformer的超大参数量语言模型（如GPT、LLaMA），具备强大的文本理解和生成能力 |
| 参数量 | 模型中可训练参数的总数，通常参数量越大，模型能力越强（需足够数据支撑） |
| 预训练 | 大模型的第一阶段训练，用海量无标注数据学习通用语言规律，得到基础模型 |
| 微调（Fine-tuning） | 预训练后的第二阶段，用小批量标注数据将基础模型适配到特定任务（如翻译、问答） |
| 指令微调（Instruction Tuning） | 用指令-响应数据微调模型，让模型理解人类指令并生成符合要求的回答，提升对齐性 |
| 对齐（Alignment） | 让大模型输出符合人类价值观、意图的技术，解决模型“胡说八道”“有害输出”问题 |
| 人类反馈强化学习（RLHF） | 对齐核心技术，通过“模型生成-人类打分-奖励模型训练-强化学习微调”四步优化模型 |
| 奖励模型（RM） | RLHF中对模型输出打分的模型，学习人类偏好，为强化学习提供奖励信号 |
| 上下文窗口 | LLM能处理的最大输入序列长度，窗口越大可理解的上下文越多（如GPT-4o为128k tokens） |
| Token | LLM的基本处理单元，可理解为“语义原子”，一个汉字/单词通常对应1-多个Token |
| 涌现能力 | 大模型参数量达到阈值后，突然出现的低参模型不具备的能力（如逻辑推理、代码生成） |
| 上下文学习（ICL） | LLM的核心能力，无需微调，仅在输入中给出示例，模型就能完成对应任务（如“1+1=2，2+2=4，3+3=？”） |
| 少样本学习（Few-shot） | ICL的子集，输入中仅给少量示例（1-10个），模型即可完成任务 |
| 零样本学习（Zero-shot） | 无需示例，模型直接根据指令完成从未训练过的任务（如让未训练过翻译的模型做中英互译） |
| 思维链（CoT） | 引导LLM分步推理的提示技术，通过“一步步思考”提升复杂任务（如数学题、逻辑题）准确率 |
| 提示工程（Prompt Engineering） | 设计最优输入提示词，让LLM高效完成任务的技术，是大模型应用的核心技能 |
| 提示词（Prompt） | 输入给LLM的指令/问题/示例，决定模型的输出方向和质量 |
| 幻觉 | LLM生成的看似合理但与事实不符的内容（如编造不存在的文献、错误数据），是大模型的核心缺陷 |
| 模型蒸馏 | 将大模型的知识迁移到小模型的技术，在保留核心能力的同时降低部署成本 |
| 量化 | 降低模型参数精度的技术（如从FP32转为INT8），减少内存占用和推理延迟，便于边缘部署 |
| 稀疏大模型 | 仅激活部分神经元/参数的大模型，在保证性能的同时提升推理效率（如GPT-4的MoE架构） |
| 混合专家模型（MoE） | 稀疏模型架构，模型包含多个“专家”子网络，输入仅激活少数相关专家，兼顾参数量和效率 |
| 多模态大模型 | 支持文本、图像、语音等多模态输入输出的大模型（如GPT-4o、文心一言4.0） |
| 开源大模型 | 开放权重和代码的大模型（如LLaMA、Qwen），支持用户二次开发和定制 |
| 闭源大模型 | 不开放权重，仅提供API调用的大模型（如GPT-4、Claude），稳定性和安全性更高 |
| 基座模型 | 仅完成预训练的基础大模型，未做指令微调或对齐，能力通用但对人类指令理解弱 |
| 对话模型 | 经指令微调和对齐的大模型，可直接用于多轮对话（如ChatGPT、豆包） |
| 代码大模型 | 专为代码生成和理解优化的LLM（如CodeLlama、GitHub Copilot），支持代码补全、调试、重构 |
| 嵌入模型（Embedding Model） | 将文本转化为高维稠密向量的模型，向量可用于检索、聚类、相似度计算，是RAG的核心 |
| 检索增强生成（RAG） | 结合检索和生成的技术，先从知识库检索相关信息，再让LLM基于检索内容生成回答，解决幻觉问题 |

# 六、数据与评估类（20个）
| 关键词 | 解说 |
|--------|------|
| 数据增强 | 对训练数据进行变换生成新样本的技术（如图像翻转、文本同义词替换），提升模型泛化能力 |
| 标注数据 | 人工标注了标签的数据集，是监督学习的核心资源，标注成本通常较高 |
| 数据清洗 | 去除数据中的噪声、缺失值、异常值的过程，保证数据质量，提升模型性能 |
| 数据采样 | 从海量数据中选取部分样本的方法（如随机采样、分层采样），平衡数据分布 |
| 类别不平衡 | 分类任务中不同类别样本数量差异极大的问题（如正样本100、负样本10000），可通过采样或加权解决 |
| 准确率（Accuracy） | 分类任务评估指标，正确预测样本数/总样本数，易受类别不平衡影响 |
| 精确率（Precision） | 预测为正类的样本中实际为正类的比例，衡量“查得准”的能力，适合垃圾邮件识别等场景 |
| 召回率（Recall） | 实际为正类的样本中被预测为正类的比例，衡量“查得全”的能力，适合疾病诊断等场景 |
| F1分数 | 精确率和召回率的调和平均，兼顾两者，适合类别不平衡任务的综合评估 |
| 混淆矩阵 | 可视化分类模型性能的矩阵，展示真实标签与预测标签的对应关系（如TP、TN、FP、FN） |
| 受试者工作特征曲线（ROC） | 以假阳性率为横轴、真阳性率为纵轴的曲线，用于评估二分类模型的整体性能 |
| 曲线下面积（AUC） | ROC曲线下的面积，取值0-1，AUC越大模型区分能力越强 |
| 均方误差（MSE） | 回归任务评估指标，预测值与真实值差值的平方和均值，衡量预测的整体误差 |
| 平均绝对误差（MAE） | 回归任务评估指标，预测值与真实值差值的绝对值均值，对异常值更鲁棒 |
| 交并比（IoU） | CV任务（目标检测/分割）评估指标，预测框与真实框的交集/并集，衡量定位精度 |
| 峰值信噪比（PSNR） | 图像生成任务评估指标，衡量生成图像与原图的相似度，值越高图像质量越好 |
| 困惑度（Perplexity） | NLP语言模型评估指标，值越低代表模型对文本的预测能力越强 |
| BLEU分数 | 机器翻译任务评估指标，衡量译文与参考译文的重合度，值越高翻译质量越好 |
| ROUGE分数 | 文本摘要任务评估指标，衡量摘要与原文的召回率，评估摘要的完整性 |
| 人工评估 | 由人类标注员对模型输出打分的评估方式，更贴合实际使用场景，常用于大模型对齐评估 |

# 七、优化与训练类（15个）
| 关键词 | 解说 |
|--------|------|
| 优化器 | 实现梯度下降的算法，负责更新模型参数，常见的有SGD、Adam、AdamW |
| 随机梯度下降（SGD） | 基础优化器，每次用单个样本的梯度更新参数，训练快但波动大 |
| 批量梯度下降（BGD） | 用全部样本的梯度更新参数，收敛稳定但计算成本高，不适合大数据集 |
| 小批量梯度下降（MBGD） | 折中SGD和BGD，每次用小批量样本更新参数，是工业界主流方案 |
| Adam | 结合动量和自适应学习率的优化器，收敛快且稳定，是深度学习的默认选择 |
| 学习率 | 控制参数更新步长的超参数，学习率过大会不收敛，过小会训练过慢 |
| 学习率调度 | 训练过程中动态调整学习率的策略（如余弦退火、阶梯下降），提升模型最终性能 |
| 动量（Momentum） | 优化器的加速策略，模拟物理惯性，让参数更新沿梯度方向“加速”，缓解震荡 |
| 批量大小（Batch Size） | 每次参数更新所用的样本数，Batch Size越大训练越稳定，但内存占用越高 |
| 正则化 | 防止模型过拟合的技术，常见的有L1正则化、L2正则化、Dropout |
| Dropout | 训练时随机关闭部分神经元的技术，让模型不依赖特定神经元，提升泛化能力 |
| 早停（Early Stopping） | 训练监控策略，当验证集性能不再提升时停止训练，避免过拟合 |
| 过采样 | 类别不平衡任务的处理方法，对少数类样本进行复制/生成，平衡数据分布 |
| 欠采样 | 类别不平衡任务的处理方法，对多数类样本进行删减，降低数据量和训练成本 |
| 分布式训练 | 多GPU/多机器并行训练模型的技术，加速超大模型和大数据集的训练，常见框架有Horovod、DeepSpeed |

# 八、工具框架与部署类（15个）
| 关键词 | 解说 |
|--------|------|
| TensorFlow | Google开源的深度学习框架，支持静态图，适合工业级部署和大规模训练 |
| PyTorch | Facebook开源的深度学习框架，支持动态图，调试友好，是科研和大模型训练的主流工具 |
| Hugging Face | 全球最大的AI模型/数据集/工具开源社区，提供Transformers、Datasets等核心库，降低大模型使用门槛 |
| Transformers | Hugging Face开源的模型库，集成了各类预训练模型（CNN、RNN、Transformer），支持多任务快速调用 |
| CUDA | NVIDIA推出的GPU并行计算框架，可加速深度学习模型的训练和推理，大幅提升效率 |
| cuDNN | NVIDIA专为深度学习优化的GPU加速库，与CUDA配合使用，优化卷积、池化等核心运算 |
| ONNX | 开放神经网络交换格式，实现模型跨框架迁移（如PyTorch模型转TensorFlow模型） |
| TensorRT | NVIDIA的推理加速引擎，通过模型优化、量化等技术提升GPU推理速度，适合部署 |
| OpenVINO | Intel推出的推理框架，优化CPU/GPU上的模型推理，适合边缘设备部署 |
| 模型部署 | 将训练好的模型集成到实际业务系统的过程，需兼顾速度、内存和稳定性 |
| 边缘计算 | 在靠近数据源头的边缘设备（如手机、摄像头）部署模型，降低延迟和带宽消耗 |
| 云推理 | 模型部署在云端服务器，通过API对外提供服务，支持弹性扩容，适合大规模应用 |
| API接口 | 模型服务的调用入口，用户可通过HTTP/GRPC等协议调用模型，无需关注底层实现 |
| 容器化 | 用Docker等工具将模型和依赖打包成容器，保证部署环境一致性，简化运维 |
| Kubernetes（K8s） | 容器编排平台，实现模型容器的自动部署、扩缩容和故障恢复，适合大规模集群管理 |

# 九、AI伦理与安全类（10个）
| 关键词 | 解说 |
|--------|------|
| AI伦理 | 研究AI技术应用中的道德规范，涵盖公平性、隐私性、责任性等维度 |
| 算法偏见 | 模型因训练数据或算法设计缺陷，对特定群体产生不公平预测（如性别/种族偏见） |
| 数据隐私 | AI应用中用户数据的保护问题，需遵循GDPR、数据安全法等法规，常用技术有联邦学习、差分隐私 |
| 联邦学习 | 分布式学习技术，数据留在本地，仅交换模型参数，实现“数据可用不可见”，保护隐私 |
| 差分隐私 | 向数据/模型中添加噪声，在保证数据可用性的同时，防止个体信息被泄露 |
| AI安全 | 防范AI模型的安全风险，包括对抗样本攻击、数据投毒、模型窃取等 |
| 对抗样本 | 对原始数据添加微小扰动生成的样本，可让模型做出错误预测（如图像贴小贴纸让CNN误判） |
| 数据投毒 | 攻击者向训练数据中注入恶意样本，导致模型性能下降或输出错误结果 |
| 模型窃取 | 攻击者通过API调用反向还原模型结构和参数，侵犯模型知识产权 |
| 深度伪造（DeepFake） | 基于GAN等技术生成的虚假图像/视频/语音（如换脸视频），存在信息欺诈风险 |

# 十、行业应用类（10个）
| 关键词 | 解说 |
|--------|------|
| 智能推荐 | 基于用户行为和偏好，为用户推荐个性化内容（如电商商品、短视频、新闻） |
| 智能客服 | 基于NLP技术的自动客服系统，可解答常见问题，分流人工客服压力 |
| 自动驾驶 | 融合CV、传感器、强化学习的AI应用，实现车辆自主感知和决策，分为L0-L5多个等级 |
| 医疗AI | 用于疾病诊断（医学影像识别）、药物研发、健康管理的AI技术，提升医疗效率和精准度 |
| 金融AI | 用于风控、量化交易、智能投顾、反欺诈的AI技术，降低金融风险，优化服务 |
| 工业AI | 应用于工业质检、设备故障预测、生产优化的AI技术，实现智能制造 |
| 教育AI | 用于个性化学习、作业批改、智能答疑的AI技术，赋能教育公平和效率 |
| 农业AI | 用于精准种植、病虫害识别、产量预测的AI技术，提升农业生产智能化水平 |
| 法律AI | 用于法条检索、案例分析、合同审核的AI技术，辅助法律从业者提升工作效率 |
| 创意生成 | 基于AI的内容创作工具，可生成文案、图像（Midjourney）、视频、音乐，赋能创意行业 |

## 结尾交付物提议
我可以帮你整理**AI核心关键词的记忆思维导图**，把这些术语按“理论-技术-工具-应用”的逻辑分层归类，方便你系统梳理知识体系，需要吗？