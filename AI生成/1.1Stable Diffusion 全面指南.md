# Stable Diffusion 全面指南
Stable Diffusion是一款开源的文本到图像（Text-to-Image）生成模型，由Stability AI联合多个机构开发，基于扩散模型（Diffusion Model）架构，支持本地部署和自定义扩展，是目前最主流的AI图像生成工具之一。


## 一、核心概念
### 1. 模型本质与原理
Stable Diffusion的核心是**扩散模型**，其工作流程可分为两个阶段：
- **前向扩散**：从清晰图像逐步添加高斯噪声，最终变成完全随机的噪声图；
- **反向扩散**：模型学习从噪声图中逐步还原出符合文本描述的清晰图像，这个过程由文本提示词（Prompt）引导。

### 2. 核心组件
一个完整的Stable Diffusion生成系统包含3个关键模块：
- **CLIP文本编码器**：将自然语言提示词转化为模型可理解的向量特征；
- **UNet扩散模型**：核心生成模块，负责在反向扩散过程中去噪并生成图像；
- **VAE（变分自编码器）**：负责图像的编码（将像素图转为潜在空间向量）和解码（将潜在向量还原为像素图），大幅降低显存占用。

### 3. 主流版本
| 版本       | 特点                                                                 | 适用场景                  |
|------------|----------------------------------------------------------------------|---------------------------|
| SD 1.5     | 生态最完善、模型/插件最多，生成人像效果优异，显存要求低（≥4GB）| 新手入门、人像/二次元创作 |
| SD 2.x     | 提升了图像分辨率和细节，支持更大尺寸生成，但人像效果略逊于SD1.5        | 风景/场景类创作           |
| SDXL（1.0）| 支持原生1024×1024高清生成，语义理解更强，细节和真实感大幅提升          | 高清商业图、复杂场景创作  |


## 二、常见部署方式
Stable Diffusion的部署方式多样，不同方式适合不同需求的用户，其中**AUTOMATIC1111 WebUI**和**ComfyUI**是最主流的两种：

### 1. AUTOMATIC1111 WebUI（新手首选）
这是最易用的图形化界面，功能集成度高，操作门槛低。
- **部署步骤**（Windows为例）：
  1. 安装Python 3.10（必须3.10版本，高版本易兼容报错）和Git；
  2. 克隆仓库：`git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`；
  3. 进入目录，双击`webui-user.bat`自动安装依赖并启动；
  4. 浏览器访问`http://127.0.0.1:7860`即可使用。
- **核心优势**：一键安装、插件丰富（ControlNet/高清修复/图生图）、操作直观。

### 2. ComfyUI（进阶用户/工作流定制）
你此前关注的ComfyUI是节点式工作流工具，更适合精细化控制生成流程。
- **核心优势**：支持自定义工作流、显存利用率更高、适合复杂任务（多模型联动/多步控制）；
- **与WebUI的区别**：WebUI是“傻瓜式”操作，ComfyUI是“专业级”节点编排，前者易用后者灵活。

### 3. 其他部署方式
- **便携版**：无需安装Python，解压即可运行，适合临时使用；
- **云端部署**：通过Colab/阿里云等云平台，无本地硬件限制。


## 三、基础使用流程（以AUTOMATIC1111为例）
1. **准备模型**
   - 下载主模型（如SD1.5的`v1-5-pruned-emaonly.safetensors`），放入`models/Stable-diffusion`目录；
   - 可额外下载LoRA（细节增强/风格迁移）、Embedding（修正人脸/去AI感）等模型，放入对应目录。

2. **输入提示词**
   - **正向提示词**：描述想要的图像，如`photorealistic portrait of a girl, natural lighting, 8K, sharp focus`；
   - **反向提示词**：描述要避免的内容，如`blurry, deformed, AI generated, extra limbs, ugly`。

3. **设置生成参数**
   - **采样器**：新手推荐`DPM++ 2M Karras`（速度快+效果稳）；
   - **步数**：15-30步（步数太少模糊，太多耗时）；
   - **CFG值**：7-10（值越高越贴合提示词，过高易出现畸形）；
   - **分辨率**：SD1.5建议512×768，SDXL建议1024×1024。

4. **生成与优化**
   - 点击**生成**按钮得到基础图像；
   - 用**高清修复**（Hires. fix）提升分辨率，用**ControlNet**控制姿势/构图。


## 四、实用优化技巧
1. **降低AI感**
   - 正向提示词添加`natural, real human, subtle details, film grain`；
   - 反向提示词添加`CGI, 3D render, plastic skin, doll`；
   - 启用**随机种子**和低CFG值（6-8），增加图像自然度。

2. **提升图像质量**
   - 搭配细节增强LoRA（如`add_detail.safetensors`）；
   - 用`ESRGAN`等超分模型进行高清放大；
   - 图生图模式：先生成低分辨率草图，再以0.3-0.5的重绘幅度优化。

3. **姿势与构图控制**
   - 安装ControlNet插件，导入OpenPose姿势图，实现精准姿势控制；
   - 用**线稿/深度图**作为ControlNet参考，固定图像构图。


## 五、模型与资源获取
- **主模型/插件**：Hugging Face（国内可通过[HF镜像站](https://hf-mirror.com/)下载）；
- **中文社区资源**：Liblibart、Civitai（国内镜像）、AI绘画论坛，可获取优质LoRA和提示词模板。

我可以帮你整理**AUTOMATIC1111 WebUI的常用插件安装清单**，需要吗？